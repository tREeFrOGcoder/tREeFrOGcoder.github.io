<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Page Draft</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

</head>
<body>
    <div class="container">


        <!-- Super large cover image section -->
        <section class="cover-image">
            <img src="src/cover.png" alt="Cover Image" class="cover-img">
            <div class="title-block">
                <h1>Final Project:</h1>
                <h2>Lightfield Camera Playground + Poisson Blending</h2>
            </div>
        </section>

        
        <!-- Sidebar with navigation -->
        <aside class="sidebar">
            <nav>
                <ul>
                    <li><a href="#proj61">Proj 6.1: Lightfield</a></li>
                    <ul>
                        <li><a href="#section11">Part 1: Refocus</a></li>
                        <li><a href="#section12">Part 2: Aperture</a></li>
                    </ul>

                    <li><a href="#proj62">Proj 6.2: Fusion</a></li>
                    <ul>
                        <li><a href="#section21">Part 1: GDF</a></li>
                        <li><a href="#section22">Part 2: Poisson</a></li>
                    </ul>

                </ul>
            </nav>
            
        </aside>

        
        <!-- Main content section -->
        <main class="content">
            
            <!-- proj61 -->
            <section id="proj61" class="section">
                <h1>Project 6.1: Lightfield Camera Playground</h1>
                <ul>
                    <li>Here we play around the beautiful focus and real camera effects that can be achieved through an array of images.</li>
                    <li>Operataions are as simple as shifting and averaging, but authentic depth-refocusing and aperture simulation can be achieved.</li>
                    <li>Using the data from: <a href=http://lightfield.stanford.edu/lfs.html>The (New) Stanford Light Field Archive</a></li>
                </ul>
                <p> </p>

                <!-- Section1 -->
                <section id="section11" class="subsection">
                    <h2>Part 1: Depth Refocusing</h2>
            
                    <ul>
                        <li>When moving the camera around, objects far away don't have as large pixel displacements as those closer to the camera.</li>
                        <li>Naive averaging over all such images will intuitively give us a final result that's "in focus".</li>
                        <li>As shown in the right image below:</li>
                    </ul>

                    <section id="image-array-large">
                        <div class="image-card-large">
                            <img src="src/clear_image.png" alt="">
                            <div class="image-single-info">
                                <p>Original Image</p>
                            </div>
                        </div>

                        <div class="image-card-large">
                            <img src="src/naive_averaged_image.png" alt="">
                            <div class="image-single-info">
                                <p>Naive Averaging Over All Images</p>
                            </div>
                        </div>
                    </section>

                    <ul>
                        <li>This observation leads to us thinking: maybe by shifting the image array appropriately,</li>
                        <li>with each image nudging a slightly different value that is a function of depth,</li>
                        <li>we can get a "refocused" image that is "in focus" at any given depth.</li>
                        <li>We shift each image using the distance between their grid coordinates and the center image location (8, 8)</li>
                        <li>Then depth is applied as a weight term that regulates the shift amount. We use simple nearest padding and bilinear interpolation.</li>
                    
                    </ul>

                    <section id="image-array-large">
                        <div class="image-card-large">
                            <img src="src/amethyst_refocusing_-2-2.gif" alt="">
                            <div class="image-single-info">
                                <p><strong>Amethyst Refocused</strong></p>
                                <p>Depth=[-2, 2], Step Size=0.2</p>
                            </div>
                        </div>

                        <div class="image-card-large">
                            <img src="src/chessboard_refocusing_-1_3.4.gif" alt="">
                            <div class="image-single-info">
                                <p><strong>Chessboard Refocused</strong></p>
                                <p>Depth=[-1, 3.4], Step Size=0.2</p>
                            </div>
                        </div>
                    </section>
                    <p> </p>

                    <ul>
            
                        <li>Additionally, if the task is run on CPU, it took over 288 minutes to just render one, so I use GPU to accelerate.</li>
                        <li>Both task below is to perform a refocused operation over depth range [-3, 3] with step size 0.2 and the image size 1400x800.</li>
                    </ul>

                    <section id="image-array-large">
                        <div class="image-card-large">
                            <img src="src/No_GPU.png" alt="">
                            <div class="image-single-info">
                                <p>Pure CPU: 218 minutes 42.3 s</p>
                            </div>
                        </div>

                        <div class="image-card-large">
                            <img src="src/GPU.png" alt="">
                            <div class="image-single-info">
                                <p><strong>GPU Accelerated: 2 minutes 22.9 s</strong></p>
                            </div>
                        </div>
                    </section>
                    

                    
                </section>
                
                <!-- Section2 -->
                <section id="section12" class="subsection">
                    <h2>Part 2: Aperture Adjustment</h2>
                    <ul>
                        <li>The key part of this section is to generate images given a Diffusion model,</li>
                        <li>Since the model is trained to denoise following a noise shedule</li>
                        <li>It's hallucination nature would result in inperfect reconstruction of the image from pure noise, which is useful for us to generating new images that also lie on the image manifold.</li>
                        <li>The key to make the generation work is to construct good sampling procedures that guide the model to denoise on the right image at the right timestep for the desired result.</li>
                    </ul>
                </section>
                
            </section>





            <!-- Proj6.1 -->
            <section id="proj62" class="section">
                <h1>Project 6.2: Gradient Domain Diffusion</h1>
                <ul>
                    <li>Here we explore the power of diffusion models mainly by implementing multiple ways of sampling, which is basically inference.</li>
                    <li>The Diffusion Model we use is DeepFloyd IF with text embeddings from TF.</li>
                    <li>Let's get started!</li>
                </ul>
                <p> </p>

                <!-- Section1 -->
                <section id="section21" class="subsection">
                    <h2>Part 1: Gradient Domain Fusion -- Toy Example</h2>
            
                    <ul>
                        <li>Below are results of taking different <code>num_inference_steps</code> values for the same prompt. </li>
                        <li><code>num_inference_steps</code> on stage 1 won't affect the quality, but affects where the set of generated images will be on the image manifold.</li>
                        <li>On the other hand, <code>num_inference_steps</code> on stage 2 DO affect the quality of the generated images.</li>
                        <li>It's also obvious that the larger the <code>num_inference_steps</code> on stage 2, the more the details. Compare image 1 and 2, image 3 and 4 to get a feeling for this.</li>
                    </ul>
                    

                    <section id="image-array-single">
                        <div class="image-card-single">
                            <img src="df/0/20-20.png" alt="Image 1">
                            <div class="image-single-info">
                                <p><strong>Stage 1: 20, Stage 2: 20</strong></p>
                            </div>
                        </div>
                    </section>

                </section>
                
                <!-- Section2 -->
                <section id="section22" class="subsection">
                    <h2>Part 2: Poisson Blending</h2>
                    <ul>
                        <li>The key part of this section is to generate images given a Diffusion model,</li>
                    </ul>
                </section>
                
            </section>







            
        </main>
    </div>
</body>
</html>
